
## 1. CUDA C++ 简介

本章通过说明CUDA编程模型中的一些基本概念在C++中的呈现方式，对这些概念进行了介绍。

本编程指南重点介绍CUDA运行时API。CUDA运行时API是在C++中使用CUDA最常用的方式，它构建在较低级别的CUDA驱动程序API之上。

[CUDA运行时API和CUDA驱动程序API]()讨论了这些API之间的区别，而CUDA驱动程序API则讨论了编写混合使用这些API的代码。


### 1.1 使用 NVCC 编译

用C++编写的GPU代码通过NVIDIA Cuda编译器nvcc进行编译。nvcc是一个编译器驱动程序，它简化了C++或PTX代码的编译过程：它提供简单且常见的命令行选项，并通过调用实现不同编译阶段的工具集来执行这些选项。

本指南将展示可在安装了CUDA工具包的任何Linux系统、Windows命令行或PowerShell，或安装了CUDA工具包的Windows子系统Linux上使用的nvcc命令行。本指南的nvcc章节涵盖了nvcc的常见用例，完整文档由nvcc用户手册提供。

### 1.2 核函数

正如《CUDA编程模型》简介中所提到的，在GPU上执行且可从主机调用的函数被称为内核。内核的编写旨在让多个并行线程同时运行。

#### 1.2.1 指定核函数

内核的代码使用```__global__```声明说明符来指定。这向编译器表明，该函数将为GPU编译，使其能够通过内核启动来调用。内核启动是一种启动内核运行的操作，通常从CPU发起。内核是具有```void```返回类型的函数。


```c++
// Kernel definition
__global__ void vecAdd(float* A, float* B, float* C)
{

}
```

#### 1.2.2 启动内核

将并行执行内核的线程数量是作为内核启动的一部分来指定的，这被称为执行配置。同一内核的不同调用可能会使用不同的执行配置，例如不同数量的线程或线程块。

从CPU代码启动内核有两种方式：三重尖括号表示法和```cudaLaunchKernelEx```。这里介绍最常用的三重尖括号表示法。使用```cudaLaunchKernelEx```启动内核的示例在第3.1.1节中进行了详细展示和讨论。

##### 1.2.2.1 三重角括号表示法

三重角括号表示法是一种用于启动内核的CUDA C++语言扩展。它被称为三重角括号，是因为它使用三个角括号字符来封装内核启动的执行配置，即```<<< >>>```。执行配置参数在角括号内以逗号分隔的列表形式指定，类似于函数调用的参数。vecAdd内核的启动语法如下所示。

```c++
__global__ void vecAdd(float* A, float* B, float* C)
 {

 }

int main()
{
    ...
    // Kernel invocation
    vecAdd<<<1, 256>>>(A, B, C);
    ...
}
```

三尖括号符号的前两个参数分别是网格维度和线程块维度。使用一维线程块或网格时，可以用整数来指定维度。


上述代码启动了一个包含256个线程的单线程块。每个线程将执行完全相同的内核代码。在线程和网格索引内置函数中，我们将展示每个线程如何利用其在线程块和网格中的索引来更改它所操作的数据。

每个块的线程数量是有限制的，因为一个块的所有线程都驻留在同一个流式多处理器（SM）上，并且必须共享该SM的资源。在当前的GPU上，一个线程块最多可以包含1024个线程。如果资源允许，多个线程块可以同时在一个SM上调度。


内核启动相对于主机线程是异步的。也就是说，内核将准备好在GPU上执行，但主机代码在继续运行之前不会等待内核在GPU上完成（甚至开始）执行。必须使用GPU和CPU之间的某种同步方式来确定内核已完成。最基本的版本是完全同步整个GPU，如[CPU与GPU同步]()中所示。更复杂的同步方法在[异步执行]()中介绍。

当使用二维或三维网格或线程块时，CUDA类型dim3被用作网格和线程块的维度参数。下面的代码片段展示了MatAdd内核的启动，它使用16×16的线程块网格，每个线程块为8×8。

```c++
int main()
{
    ...
    dim3 grid(16,16);
    dim3 block(8,8);
    MatAdd<<<grid, block>>>(A, B, C);
    ...
}
```

#### 1.2.3 线程和网格索引内在函数

在核函数代码中，CUDA 提供了内在函数来访问执行配置的参数以及线程或块的索引。

+ ```threadIdx``` 提供线程在其线程块内的索引。线程块中的每个线程都有不同的索引。

+ ```blockDim``` 给出线程块的维度，这是在内核启动的执行配置中指定的

+ ```blockIdx``` 给出了线程块在网格中的索引。每个线程块都有不同的索引

+ ```gridDim``` 给出网格的维度，这是在启动内核时的执行配置中指定的


这些内部变量中的每一个都是一个3分量向量，具有```.x```、```.y```和```.z```成员。启动配置未指定的维度将默认为1。```threadIdx```和```blockIdx```是从0开始索引的。也就是说，```threadIdx.x```的值范围是从0到```blockDim.x-1```（含）。```.y```和```.z```在其各自的维度中以相同方式运作。

同样，```blockIdx.x``` 的值将从 0 开始，一直到并包括 ```gridDim.x-1```，```.y```和 ```.z``` 维度的情况也分别与此相同。


这些允许单个线程确定它应该执行的工作。回到```vecAdd```内核，该内核接受三个参数，每个参数都是一个浮点数向量。内核对```A```和```B```执行逐元素加法，并将结果存储在```C```中。内核是并行化的，每个线程将执行一次加法。它计算哪个元素由其线程和网格索引决定。

```c++
__global__ void vecAdd(float* A, float* B, float* C)
{
   // calculate which element this thread is responsible for computing
   int workIndex = threadIdx.x + blockDim.x * blockIdx.x

   // Perform computation
   C[workIndex] = A[workIndex] + B[workIndex];
}

int main()
{
    ...
    // A, B, and C are vectors of 1024 elements
    vecAdd<<<4, 256>>>(A, B, C);
    ...
}
```

在这个示例中，使用了4个线程块，每个线程块包含256个线程，来对一个有1024个元素的向量进行加法运算。在第一个线程块中，```blockIdx.x```为0，因此每个线程的```workIndex```就是其```threadIdx.x```。在第二个线程块中，
```blockIdx.x```为1，所以```blockDim.x * blockIdx.x```与```blockDim.x```相等，在这种情况下是256。第二个线程块中每个线程的```workIndex```为其```threadIdx.x + 256```。在第三个线程块中，```workIndex```为```threadIdx.x + 512```。

这种对```workIndex```的计算在一维并行化中非常常见。扩展到二维或三维时，在这些维度的每一个中通常都遵循相同的模式。

##### 1.2.3.1 边界检查

上面给出的示例假设向量的长度是线程块大小的倍数，在本例中为256个线程。为了让内核能够处理任意向量长度，我们可以添加检查，确保内存访问不会超出如下所示的数组边界，然后启动一个线程块，该线程块会包含一些不活跃的线程。

```c++
__global__ void vecAdd(float* A, float* B, float* C, int vectorLength)
{
     // calculate which element this thread is responsible for computing
     int workIndex = threadIdx.x + blockDim.x * blockIdx.x

     if(workIndex < vectorLength)
     {
         // Perform computation
         C[workIndex] = A[workIndex] + B[workIndex];
     }
}
```

使用上述内核代码，可以启动比所需数量更多的线程，而不会导致数组的越界访问。当```workIndex```超过```vectorLength```时，线程会退出且不执行任何工作。在一个块中启动不执行工作的额外线程不会带来很大的开销成本，但应避免启动其中没有线程执行工作的线程块。
现在，这个内核能够处理向量长度不是块大小倍数的情况。

所需线程块的数量可以通过对所需线程数（在本例中为向量长度）除以每个块的线程数取上整来计算。也就是说，将所需线程数除以每个块的线程数，再向上取整。下面给出了一种用单次整数除法来表示这一计算的常用方法。通过在整数除法前加上```threads - 1```，这一运算就起到了上整函数的作用，
只有当向量长度不能被每个块的线程数整除时，才会额外增加一个线程块。

```c++
// vectorLength is an integer storing number of elements in the vector
int threads = 256;
int blocks = (vectorLength + threads-1)/threads;
vecAdd<<<blocks, threads>>>(devA, devB, devC, vectorLength);
```

[CUDA核心计算库（CCCL）]()提供了一个便捷的工具```cuda::ceil_div```，用于执行这种向上取整除法，以计算内核启动所需的块数。通过包含头文件```<cuda/cmath>```可以使用此工具。

```c++
// vectorLength is an integer storing number of elements in the vector
int threads = 256;
int blocks = cuda::ceil_div(vectorLength, threads);
vecAdd<<<blocks, threads>>>(devA, devB, devC, vectorLength);
```

这里选择每个块256个线程是任意的，但这通常是一个不错的初始值。

## 3. GPU计算中的内存

为了使用上面所示的```vecAdd```内核，数组A、B和C必须位于GPU可访问的内存中。有几种不同的方法可以实现这一点，这里将介绍其中两种。其他方法将在后面关于统一内存的部分中介绍。在GPU上运行的代码可使用的内存空间已在GPU内存中介绍，并将在GPU设备内存空间中更详细地阐述。

### 3.1. 统一内存

统一内存是CUDA运行时的一项功能，它允许NVIDIA驱动程序管理主机和设备之间的数据移动。内存通过```cudaMallocManaged API```分配，或者通过使用```__managed__```说明符声明变量来分配。NVIDIA驱动程序会确保，无论GPU还是CPU尝试访问内存，该内存都能被它们访问到。

下面的代码展示了一个完整的函数，用于启动```vecAdd```内核，该内核使用统一内存来存储将在GPU上使用的输入和输出向量。```cudaMallocManaged```分配可从CPU或GPU访问的缓冲区。这些缓冲区使用```cudaFree```释放。

```c++
void unifiedMemExample(int vectorLength)
{
    // Pointers to memory vectors
    float* A = nullptr;
    float* B = nullptr;
    float* C = nullptr;
    float* comparisonResult = (float*)malloc(vectorLength*sizeof(float));

    // Use unified memory to allocate buffers
    cudaMallocManaged(&A, vectorLength*sizeof(float));
    cudaMallocManaged(&B, vectorLength*sizeof(float));
    cudaMallocManaged(&C, vectorLength*sizeof(float));

    // Initialize vectors on the host
    initArray(A, vectorLength);
    initArray(B, vectorLength);

    // Launch the kernel. Unified memory will make sure A, B, and C are
    // accessible to the GPU
    int threads = 256;
    int blocks = cuda::ceil_div(vectorLength, threads);
    vecAdd<<<blocks, threads>>>(A, B, C, vectorLength);
    // Wait for the kernel to complete execution
    cudaDeviceSynchronize();

    // Perform computation serially on CPU for comparison
    serialVecAdd(A, B, comparisonResult, vectorLength);

    // Confirm that CPU and GPU got the same answer
    if(vectorApproximatelyEqual(C, comparisonResult, vectorLength))
    {
        printf("Unified Memory: CPU and GPU answers match\n");
    }
    else
    {
        printf("Unified Memory: Error - CPU and GPU answers do not match\n");
    }

    // Clean Up
    cudaFree(A);
    cudaFree(B);
    cudaFree(C);
    free(comparisonResult);

}
```

所有支持CUDA的操作系统和GPU均支持统一内存，不过其底层机制和性能可能会因系统架构而有所不同。[统一内存]()提供了更多详细信息。在某些Linux系统上（例如那些配备地址转换服务或异构内存管理的系统），所有系统内存都会自动成为统一内存，无需使用```cudaMallocManaged```或```__managed__```说明符。

#### 3.2 显式内存管理

显式管理内存分配以及内存空间之间的数据迁移有助于提高应用程序性能，不过这确实会使代码更为冗长。下面的代码使用```cudaMalloc```在GPU上显式分配内存。GPU上的内存使用与前面示例中统一内存所使用的相同的```cudaFree``` API来释放。

```c++
void explicitMemExample(int vectorLength)
{
    // Pointers for host memory
    float* A = nullptr;
    float* B = nullptr;
    float* C = nullptr;
    float* comparisonResult = (float*)malloc(vectorLength*sizeof(float));
    
    // Pointers for device memory
    float* devA = nullptr;
    float* devB = nullptr;
    float* devC = nullptr;

    //Allocate Host Memory using cudaMallocHost API. This is best practice
    // when buffers will be used for copies between CPU and GPU memory
    cudaMallocHost(&A, vectorLength*sizeof(float));
    cudaMallocHost(&B, vectorLength*sizeof(float));
    cudaMallocHost(&C, vectorLength*sizeof(float));

    // Initialize vectors on the host
    initArray(A, vectorLength);
    initArray(B, vectorLength);

    // start-allocate-and-copy
    // Allocate memory on the GPU
    cudaMalloc(&devA, vectorLength*sizeof(float));
    cudaMalloc(&devB, vectorLength*sizeof(float));
    cudaMalloc(&devC, vectorLength*sizeof(float));

    // Copy data to the GPU
    cudaMemcpy(devA, A, vectorLength*sizeof(float), cudaMemcpyDefault);
    cudaMemcpy(devB, B, vectorLength*sizeof(float), cudaMemcpyDefault);
    cudaMemset(devC, 0, vectorLength*sizeof(float));
    // end-allocate-and-copy

    // Launch the kernel
    int threads = 256;
    int blocks = cuda::ceil_div(vectorLength, threads);
    vecAdd<<<blocks, threads>>>(devA, devB, devC);
    // wait for kernel execution to complete
    cudaDeviceSynchronize();

    // Copy results back to host
    cudaMemcpy(C, devC, vectorLength*sizeof(float), cudaMemcpyDefault);

    // Perform computation serially on CPU for comparison
    serialVecAdd(A, B, comparisonResult, vectorLength);

    // Confirm that CPU and GPU got the same answer
    if(vectorApproximatelyEqual(C, comparisonResult, vectorLength))
    {
        printf("Explicit Memory: CPU and GPU answers match\n");
    }
    else
    {
        printf("Explicit Memory: Error - CPU and GPU answers to not match\n");
    }

    // clean up
    cudaFree(devA);
    cudaFree(devB);
    cudaFree(devC);
    cudaFreeHost(A);
    cudaFreeHost(B);
    cudaFreeHost(C);
    free(comparisonResult);
}
```

CUDA API 中的 ```cudaMemcpy``` 用于将数据从位于 CPU 上的缓冲区复制到位于 GPU 上的缓冲区。除了目标指针、源指针和以字节为单位的大小外，```cudaMemcpy``` 的最后一个参数是 ```cudaMemcpyKind_t``。
它可以取诸如 ```cudaMemcpyHostToDevice```（用于从 CPU 复制到 GPU）、```cudaMemcpyDeviceToHost```（用于从 GPU 复制到 CPU）或 ```cudaMemcpyDeviceToDevice```（用于 GPU 内部或 GPU 之间的复制）等值。

在这个示例中，```cudaMemcpyDefault``` 被作为最后一个参数传递给 ```cudaMemcpy```。这会使 CUDA 利用源指针和目标指针的值来确定要执行的复制类型。

```cudaMemcpy``` API 是同步的。也就是说，在复制完成之前，它不会返回。异步复制将在 在 CUDA 流中启动内存传输 中介绍。

这段代码使用```cudaMallocHost```在CPU上分配内存。这会在主机上分配**页锁定内存**，这种内存可以提高复制性能，并且是异步内存传输所必需的。通常，对于将用于与GPU之间进行数据传输的CPU缓冲区，使用页锁定内存是一种很好的做法。
在某些系统上，如果过多的主机内存被页锁定，性能可能会下降。最佳实践是仅对将用于向GPU发送数据或从GPU接收数据的缓冲区进行页锁定。

#### 3.3. 内存管理与应用程序性能

从上面的例子可以看出，显式内存管理更为繁琐，它要求程序员指定主机和设备之间的数据复制操作。这既是显式内存管理的优势，也是其劣势：它让程序员能够更好地控制主机与设备之间数据复制的时机、内存的驻留位置，以及具体在何处分配何种内存。
显式内存管理通过控制内存传输并将其与其他计算重叠进行，能够带来性能提升的可能。
