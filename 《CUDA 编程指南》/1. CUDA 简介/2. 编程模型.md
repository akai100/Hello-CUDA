
## 1. 异构系统

CUDA编程模型假定存在一个异构计算系统，这意味着该系统同时包含GPU和CPU。CPU及其直接连接的内存分别被称为**主机**和**主机内存**。GPU及其直接连接的内存分别被称为**设备**和**设备内存**。在一些片上系统（SoC）中，这些可能是单个封装的一部分。在更大的系统中，可能会有多个CPU或GPU。


CUDA应用程序会在GPU上执行部分代码，但应用程序的执行总是从CPU开始。主机代码（即运行在CPU上的代码）可以使用CUDA API在主机内存和设备内存之间复制数据、启动在GPU上执行的代码，以及等待数据复制或GPU代码完成。CPU和GPU可以同时执行代码，通常通过最大限度地提高CPU和GPU的利用率来获得最佳性能。


应用程序在GPU上执行的代码被称为**设备代码**，而由于历史原因，在GPU上调用执行的函数被称为**内核**。启动内核运行的操作称为**启动内核**。可以将内核启动理解为启动许多线程在GPU上并行执行内核代码。

## 2. 硬件模型

与任何编程模型一样，CUDA 依赖于底层硬件的概念模型。就 CUDA 编程而言，GPU 可被视为一组流式多处理器（SM）的集合，这些流式多处理器被组织成称为图形处理集群（GPC）的组。每个 SM 包含一个本地寄存器文件、一个统一数据缓存，以及多个执行计算的功能单元。
统一数据缓存为共享内存和 L1 缓存提供物理资源。统一数据缓存对 L1 缓存和共享内存的分配可在运行时进行配置。不同 GPU 架构中，SM 内不同类型内存的大小以及功能单元的数量可能有所不同。

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/gpu-cpu-system-diagram.png)

### 2.1 线程快和网格

当应用程序启动内核时，会启动许多线程，通常达数百万个。这些线程被组织成块。一组线程被称为**线程块**，这或许并不令人意外。线程块被组织成一个网格。一个网格中的所有线程块都具有相同的大小和维度。下图展示了线程快网格的示意图：

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/grid-of-thread-blocks.png)


线程块和网格可以是一维、二维或三维的。这些维度有助于简化单个线程到工作单元或数据项的映射。

启动内核时，会使用特定的执行配置来启动，该配置指定了网格和线程块的维度。执行配置还可能包含可选参数，如集群大小、流和SM配置设置。

通过使用内置变量，每个执行内核的线程都能确定自己在其所属线程块中的位置，以及该线程块在所属网格中的位置。线程还可以利用这些内置变量来确定启动内核时所使用的线程块和网格的维度。这使得每个线程在所有运行该内核的线程中都具有唯一的标识。这种标识常被用于确定线程负责处理哪些数据或执行哪些操作。

线程块中的所有线程都在单个SM中执行。这使得线程块内的线程能够高效地相互通信和同步。线程块内的所有线程都可以访问片上共享内存，该内存可用于在线程块的线程之间交换信息。

一个网格可能由数百万个线程块组成，而执行该网格的GPU可能只有几十个或几百个SM。一个线程块的所有线程都由单个SM执行，并且在大多数情况下[1]，会在该SM上运行至完成。线程块之间的调度无法得到保证，因此一个线程块不能依赖其他线程块的结果，因为其他线程块可能要等到该线程块完成后才能被调度。

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/thread-block-scheduling.png)

CUDA编程模型支持在任意规模的GPU上运行任意大的网格，无论GPU只有一个SM还是数千个SM。为实现这一点，除了一些例外情况，CUDA编程模型要求不同线程块中的线程之间不存在数据依赖关系。
也就是说，一个线程不应依赖同一网格中其他线程块内线程的计算结果，也不应与之同步。一个线程块内的所有线程会同时在同一个SM上运行。网格中的不同线程块会在可用的SM之间进行调度，
并且可以按任意顺序执行。简而言之，CUDA编程模型要求线程块能够以任意顺序、并行或串行的方式执行。

#### 2.1.1 线程快集群

除线程块外，计算能力为9.0及更高版本的GPU有一种可选的分组级别，称为集群。集群是一组线程块，与线程块和网格一样，它们可以按1、2或3维布局。下图展示了一个线程块网格，该网格也被组织成集群。指定集群不会改变网格维度或网格内线程块的索引。

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/grid-of-clusters.png)

指定集群会将相邻的线程块分组到集群中，并在集群级别提供一些额外的同步和通信机会。具体来说，一个集群中的所有线程块都在单个GPC中执行。下图展示了在指定集群时，线程块如何被调度到GPC中的SM上。由于线程块是同时调度的，并且处于单个GPC内，
同一集群中不同块的线程可以使用协作组提供的软件接口进行相互通信和同步。集群中的线程可以访问该集群内所有块的共享内存，这被称为分布式共享内存。集群的最大大小取决于硬件，并且在不同设备之间有所不同。

![图 当指定集群时，集群中的线程块会按其集群形状在网格内排列。一个集群的线程块会同时在单个GPC的SM上进行调度](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/thread-block-scheduling-with-clusters.png)

#### 2.1.2 线程束和SIMT

在一个线程块内，线程被组织成每组32个线程的单元，称为**线程束**。线程束以单指令多线程（SIMT）范式执行内核代码。在SIMT中，线程束中的所有线程都在执行相同的内核代码，但每个线程可能通过代码遵循不同的分支。也就是说，尽管程序的所有线程都执行相同的代码，但线程不需要遵循相同的执行路径。

当线程由线程束执行时，它们会被分配一个线程束通道。线程束通道的编号为0到31，线程块中的线程会以硬件多线程中详述的可预测方式分配给线程束。

warp中的所有线程会同时执行相同的指令。如果warp中的一些线程在执行时走了某个控制流分支，而其他线程没有，那么在执行走该分支的线程时，不走该分支的线程会被屏蔽。例如，若某个条件仅对warp中一半的线程为真，那么在活跃线程执行这些指令时，warp中的另一半线程会被屏蔽。这种情况如图7所示。
当warp中的不同线程走不同的代码路径时，有时会被称为warp分化。因此，当warp中的线程遵循相同的控制流路径时，GPU的利用率能达到最大化。

![](https://docs.nvidia.com/cuda/cuda-programming-guide/_images/active-warp-lanes.png)

在SIMT模型中，一个线程束中的所有线程都以锁步方式在核函数中推进。硬件执行可能有所不同。有关这种区别的重要性，请参见独立线程执行部分的更多信息。不建议利用线程束执行实际如何映射到真实硬件的相关知识。

CUDA编程模型和SIMT规定，一个线程束中的所有线程共同推进代码执行。只要遵循编程模型，硬件可以以对程序透明的方式优化被屏蔽的线程通道。如果程序违反此模型，可能会导致未定义行为，且在不同的GPU硬件上可能会有所不同。

虽然在编写CUDA代码时无需考虑线程束，但理解线程束执行模型有助于理解诸如全局内存合并和共享内存存储体访问模式等概念。一些高级编程技术会利用线程块内线程束的特性来减少线程分支并最大限度地提高利用率。这种优化及其他优化手段，都运用了线程在执行时会被分组为线程束这一知识。

warp执行的一个含义是，线程块最好指定为线程总数是32的倍数。使用任意数量的线程都是合法的，但当总数不是32的倍数时，线程块的最后一个warp会有一些通道在整个执行过程中未被使用。这可能会导致该warp的功能单元利用率和内存访问不够理想。

## 3. GPU 内存

### 3.1 异构系统中的 DRAM 内存

GPU和CPU都有直接连接的DRAM芯片。在配备多个GPU的系统中，每个GPU都有自己的内存。从设备代码的角度来看，连接到GPU的DRAM被称为**全局内存**，因为它可被GPU中的所有SM访问。这一术语并不意味着它必然能在系统内的所有位置被访问。连接到CPU的DRAM被称为系统内存或主机内存。

与中央处理器（CPU）类似，图形处理器（GPU）也采用**虚拟内存寻址**。在所有当前支持的系统上，CPU和GPU使用单一的统一虚拟内存空间。这意味着系统中每个GPU的虚拟内存地址范围都是唯一的，且与CPU以及系统中其他所有GPU的地址范围截然不同。对于某个给定的虚拟内存地址，
我们能够确定该地址是位于GPU内存还是系统内存中；而在配备多个GPU的系统上，还能确定该地址位于哪个GPU的内存中。

有一些CUDA API可用于分配GPU内存、CPU内存，以及在CPU和GPU上的分配内存之间、同一GPU内或多GPU系统中的多个GPU之间进行复制。如果需要，可以显式控制数据的位置。

### 3.2 GPU 中的片上存储器

除了全局内存外，每个GPU都有一些片上内存。每个SM都有自己的寄存器文件和共享内存。这些内存是SM的一部分，在SM内执行的线程可以极快地访问它们，但运行在其他SM中的线程无法访问这些内存。

寄存器文件存储线程局部变量，这些变量通常由编译器分配。共享内存可被线程块或集群内的所有线程访问。共享内存可用于在线程块或集群的线程之间交换数据。

SM中的寄存器文件和统一数据缓存的大小是有限的。SM的寄存器文件、统一数据缓存的大小，以及如何配置统一数据缓存以实现L1缓存和共享内存的平衡，可在*每个计算能力的内存信息**中找到。寄存器文件、共享内存空间和L1缓存由线程块中的所有线程共享。

要将线程块调度到SM上，每个线程所需的寄存器总数乘以线程块中的线程数必须小于或等于SM中的可用寄存器数。如果一个线程块所需的寄存器数量超过了寄存器文件的大小，那么该内核无法启动，必须减少线程块中的线程数量，才能使线程块可以启动。

共享内存分配在线程块级别进行。也就是说，与按线程分配的寄存器不同，共享内存的分配对整个线程块来说是通用的。

#### 3.2.1 缓存

除了可编程存储器外，GPU 还具有 L1 和 L2 缓存。每个 SM 都有一个 L1 缓存，它是统一数据缓存的一部分。更大的 L2 缓存由 GPU 内的所有 SM 共享。这一点可以在 图 2 的 GPU 框图中看到。每个 SM 还有一个单独的 常量缓存，用于缓存全局内存中已声明在核函数生命周期内为常量的值。
编译器也可能将核函数参数放入常量内存中。通过允许将核函数参数缓存在 SM 中且与 L1 数据缓存分开，这有助于提高核函数的性能。

### 3.3 统一内存

当应用程序在GPU或CPU上显式分配内存时，该内存只能被运行在该设备上的代码访问。也就是说，CPU内存只能被CPU代码访问，而GPU内存只能被运行在GPU上的内核访问[2]。用于在CPU和GPU之间复制内存的CUDA API用于在适当的时间将数据显式复制到正确的内存中。

CUDA的一项名为统一内存的功能允许应用程序进行可从CPU或GPU访问的内存分配。CUDA运行时或底层硬件会在需要时实现访问或将数据迁移到正确的位置。即使使用统一内存，要获得最佳性能，也应尽量减少内存迁移，并尽可能从直接连接到数据所在内存的处理器访问数据。

系统的硬件特性决定了内存空间之间的数据访问和交换方式。统一内存部分介绍了统一内存系统的不同类别。统一内存部分包含了更多关于统一内存在各种情况下的使用和行为的详细信息。


